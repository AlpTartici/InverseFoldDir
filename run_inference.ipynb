{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2825d26a",
   "metadata": {},
   "source": [
    "# Protein Sequence Sampling & Inpainting Notebook\n",
    "\n",
    "## User Guide\n",
    "\n",
    "This notebook allows you to easily generate protein sequences using our trained Dirichlet Flow Matching (DFM) model. You can run either:\n",
    "\n",
    "### **Mode 1: Full Sequence Sampling**\n",
    "- Generate completely new sequences for given protein structures\n",
    "- Input: PDB ID (e.g., \"1abc\", \"1fcd.C\") or PDB file path\n",
    "- Output: New amino acid sequences that should fold to the input structure\n",
    "\n",
    "### **Mode 2: Sequence Inpainting** \n",
    "- Predict specific amino acids while keeping others fixed\n",
    "- Input: PDB ID + positions to predict OR template sequence with 'X' for unknowns\n",
    "- Output: Completed sequences with predictions for masked positions\n",
    "\n",
    "### **How to Use:**\n",
    "1. **Choose your mode** by setting `mode = \"sampling\"` or `mode = \"inpainting\"`\n",
    "2. **Set your PDB inputs** in the `pdb_inputs` list (e.g., `[\"1abc\", \"1fcd.C\"]`)\n",
    "3. **For inpainting mode**: specify either `mask_positions` or `template_sequence`\n",
    "4. **Run all cells** - the notebook will handle everything else automatically\n",
    "5. **Check results** in the output directory and the displayed results\n",
    "\n",
    "### **Advanced Parameters:**\n",
    "- `flow_temp`: Temperature for sampling (0.1-1.0, lower = more conservative)\n",
    "- `steps`: Number of sampling steps (10-50, more = higher quality but slower)\n",
    "- `ensemble_size`: Number of structural variants (1-10, more = better consensus)\n",
    "- `structure_noise_mag_std`: Structural noise level (0.0-2.0, higher = more structural diversity)\n",
    "\n",
    "### **Outputs:**\n",
    "- **Console**: Sequences printed with confidence scores\n",
    "- **CSV files**: Detailed results in `{output_dir}/sampling_results.csv`\n",
    "- **Trajectory files**: For detailed analysis (when sampling <4 proteins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cb926ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n",
      "Using device: GPU\n",
      "Working directory: /home/t-alptartici/inverse-folding\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS AND SETUP\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the current directory to Python path\n",
    "current_dir = os.getcwd()\n",
    "if current_dir not in sys.path:\n",
    "    sys.path.append(current_dir)\n",
    "\n",
    "# Import our sampling and inpainting modules\n",
    "from training.sample_utils import (\n",
    "    load_model_distributed, process_input_specification, CustomInputDataset,\n",
    "    compute_sampling_metrics, save_results_to_files, IDX_TO_AA, THREE_TO_ONE\n",
    ")\n",
    "from training.sample import sample_chain\n",
    "from training.inpainting import sample_chain_inpainting\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"Using device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"Working directory: {current_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baf3a9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded!\n",
      "   Mode: sampling\n",
      "   Input proteins: ['1fcd.C', '1h2s.B']\n",
      "   Ensemble size: 3\n",
      "   Output directory: ./inference_output\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# USER CONFIGURATION - MODIFY THESE SETTINGS\n",
    "# =============================================================================\n",
    "\n",
    "# ===== SAMPLING MODE =====\n",
    "# Choose: \"sampling\" (generate new sequences) or \"inpainting\" (predict masked positions)\n",
    "mode = \"sampling\"  # Change to \"inpainting\" for inpainting mode\n",
    "\n",
    "# ===== INPUT PROTEINS =====\n",
    "# List of PDB IDs or file paths to process\n",
    "# Examples: [\"1abc\"], [\"1fcd.C\"], [\"1abc\", \"1def.A\"], [\"/path/to/protein.pdb\"]\n",
    "pdb_inputs = [\"1fcd.C\", \"1h2s.B\"]  # MODIFY THIS: Add your PDB IDs here\n",
    "verbose = False\n",
    "\n",
    "# ===== INPAINTING SETTINGS (only used if mode=\"inpainting\") =====\n",
    "# Option 1: Specify positions to predict (0-indexed, in structure sequence)\n",
    "mask_positions = [10, 15, 20, 25, 30]  # MODIFY THIS: Positions to predict\n",
    "\n",
    "# Option 2: Use template sequence with 'X' for positions to predict\n",
    "# Set to None to use mask_positions instead\n",
    "template_sequence = None  # Example: \"ACDEFXHIKLXNPQXSTVWY\" \n",
    "\n",
    "# Option 3: Random masking percentage (only if above are None)\n",
    "#mask_ratio = 0.15  # 15% of positions will be randomly masked\n",
    "\n",
    "# ===== SAMPLING PARAMETERS =====\n",
    "flow_temp = 0.1             # Temperature (0.1-1.0, lower = more conservative)\n",
    "steps = 20                   # Number of sampling steps (10-50)\n",
    "max_time = 8.0              # Maximum noise level\n",
    "min_time = 0.0              # Minimum noise level  \n",
    "dirichlet_concentration = 1.0 # Initial distribution concentration\n",
    "\n",
    "# ===== ENSEMBLE PARAMETERS =====\n",
    "ensemble_size = 3                 # Number of structural variants (1-10)\n",
    "ensemble_consensus_strength = 1.0    # Consensus strength (0=independent, 1=full consensus)\n",
    "structure_noise_mag_std = 0.1        # Structural noise level (0.0-2.0 Ã…)\n",
    "uncertainty_struct_noise_scaling = False  # Scale noise by uncertainty\n",
    "\n",
    "# ===== OUTPUT SETTINGS =====\n",
    "output_dir = \"./inference_output\"   # Output directory\n",
    "save_probabilities = True          # Save detailed probability distributions\n",
    "verbose = True                     # Print detailed information\n",
    "\n",
    "# ===== MODEL AND DATA PATHS =====\n",
    "model_path = \"ckpts/model_316.pt\"\n",
    "split_json = \"datasets/cath-4.2/chain_set_splits.json\"\n",
    "map_pkl = \"datasets/cath-4.2/chain_set_map_with_b_factors_dssp.pkl\"\n",
    "\n",
    "print(\"Configuration loaded!\")\n",
    "print(f\"   Mode: {mode}\")\n",
    "print(f\"   Input proteins: {pdb_inputs}\")\n",
    "print(f\"   Ensemble size: {ensemble_size}\")\n",
    "print(f\"   Output directory: {output_dir}\")\n",
    "if mode == \"inpainting\":\n",
    "    if template_sequence:\n",
    "        print(f\"   Template sequence: {template_sequence}\")\n",
    "    elif mask_positions:\n",
    "        print(f\"   Positions to predict: {mask_positions}\")\n",
    "    else:\n",
    "        print(f\"   Random masking: {mask_ratio*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec3a13b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created output directory: ./inference_output\n",
      "Using device: cuda\n",
      "Model path validated: ckpts/model_316.pt\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t-alptartici/miniconda3/envs/inv_fold_dir/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: ckpts/model_316.pt\n",
      "============================================================\n",
      "CHECKPOINT PARAMETER EXTRACTION\n",
      "============================================================\n",
      "Found 'args' in checkpoint - extracting model configuration\n",
      "Args type: dict\n",
      "\n",
      "All available parameters in checkpoint args:\n",
      "  af2_chunk_dir: /mnt/default/data/inv_fold/datasets/af2_chunks_with_dssp/\n",
      "  af2_chunk_limit: None\n",
      "  alpha_max: 10.0\n",
      "  alpha_min: 1.0\n",
      "  alpha_range: 1.0\n",
      "  alpha_spacing: 0.01\n",
      "  alternating_pure_batches: False\n",
      "  architecture: interleaved\n",
      "  batch: 32\n",
      "  checkpoint_copy_dir: /tmp\n",
      "  config_file: None\n",
      "  debug_mode: False\n",
      "  device: auto\n",
      "  dirichlet_multiplier_training: 1.0\n",
      "  disable_time_conditioning: False\n",
      "  distributed: False\n",
      "  dropout: 0.0\n",
      "  edge_dim_s: 32\n",
      "  edge_dim_v: 1\n",
      "  enable_checkpoint_rollback: True\n",
      "  epoch_timeout: 4200.0\n",
      "  epochs: 500\n",
      "  external_checkpoint: None\n",
      "  fail_fast: False\n",
      "  flexible_loss_scaling: False\n",
      "  grad_clip: 5.0\n",
      "  head_hidden: 256\n",
      "  heterogeneous_batches: True\n",
      "  hidden_dim: 640\n",
      "  hidden_dim_v: 80\n",
      "  hybrid_fail_fast: False\n",
      "  hybrid_max_af2_errors: 10\n",
      "  hybrid_max_pdb_errors: 10\n",
      "  k_farthest: None\n",
      "  k_neighbors: None\n",
      "  k_random: None\n",
      "  label_similarity_csv: ../df_combined_for_one_hot.csv\n",
      "  lambda_dssp_loss: 0.5\n",
      "  local_rank: 0\n",
      "  lr: 0.0001\n",
      "  map_pkl: /workspace/datasets/cath-4.2/chain_set_map_with_b_factors_dssp.pkl\n",
      "  max_batches_per_epoch: 500\n",
      "  max_edge_dist: 10.0\n",
      "  max_error_count: 10\n",
      "  min_lr_stop_point: 1e-11\n",
      "  model_name_prefix: dfm_model\n",
      "  node_dim_s: 10\n",
      "  node_dim_v: 3\n",
      "  num_layers_gvp: 21\n",
      "  num_layers_prediction: 6\n",
      "  num_message_layers: 20\n",
      "  num_rbf_3d: 16\n",
      "  num_rbf_seq: 16\n",
      "  num_workers: 10\n",
      "  output_dir: /mnt/output/projects/inv_fold/amlt-results/7243388880.06502-bcdc3a09-42e5/config_316_20250831_033620\n",
      "  overfit_protein_name: None\n",
      "  patience: 400\n",
      "  pdb_directory: ../datasets/all_chain_pdbs/\n",
      "  penalize_x_prediction: True\n",
      "  ratio_af2_pdb: 31\n",
      "  rbf_3d_max: 20.0\n",
      "  rbf_3d_min: 1.0\n",
      "  rbf_3d_spacing: exponential\n",
      "  recycle_steps: 1\n",
      "  resume_from_checkpoint: /workspace/best_model_301.pt\n",
      "  rollback_patience_factor: 0.51\n",
      "  run_mode: regular\n",
      "  sampling_only: False\n",
      "  save_intermediate_models: True\n",
      "  schedule_on_train_loss: False\n",
      "  scheduler_patience: 12\n",
      "  seed: 42\n",
      "  split_json: /workspace/datasets/cath-4.2/chain_set_splits.json\n",
      "  struct_noise_in_sampling: False\n",
      "  structure_noise_mag_std: 0.25\n",
      "  t_max: 8.0\n",
      "  t_min: 0.0\n",
      "  time_based_struct_noise: increasing\n",
      "  time_dim: 64\n",
      "  time_integration: film\n",
      "  time_sampling_strategy: exponential\n",
      "  time_scale: 1.0\n",
      "  uncertainty_struct_noise_scaling: True\n",
      "  use_checkpoint_opt_and_sched: False\n",
      "  use_qkv: True\n",
      "  use_smoothed_labels: False\n",
      "  use_virtual_node: True\n",
      "  use_wandb: True\n",
      "  val_batch: 32\n",
      "  val_metric: val_all\n",
      "  verbose: False\n",
      "  wandb_project: inverse-folding\n",
      "  weight_decay: 0.001\n",
      "\n",
      "Key model architecture parameters:\n",
      "  num_layers_gvp: 21\n",
      "  num_layers_prediction: 6\n",
      "  hidden_dim: 640\n",
      "  use_qkv: True\n",
      "  time_dim: 64\n",
      "  use_virtual_node: True\n",
      "  node_dims: (10, 3)\n",
      "  edge_dims: (32, 1)\n",
      "  hidden_dims: (640, 80)\n",
      "  dropout: 0.0\n",
      "\n",
      "Dataset-related parameters:\n",
      "Using dataset parameters from dedicated graph_builder_params section\n",
      "  split_json: /workspace/datasets/cath-4.2/chain_set_splits.json\n",
      "  map_pkl: /workspace/datasets/cath-4.2/chain_set_map_with_b_factors_dssp.pkl\n",
      "  use_virtual_node: True\n",
      "  max_length: None\n",
      "  use_graph_builder: None\n",
      "\n",
      "Graph building parameters:\n",
      "Found dedicated graph_builder_params section in checkpoint\n",
      "Found dedicated model_architecture_params section in checkpoint\n",
      "Model architecture parameters from checkpoint:\n",
      "  hidden_dim: 640\n",
      "  hidden_dim_v: 80\n",
      "  node_dim_s: 10\n",
      "  node_dim_v: 3\n",
      "  edge_dim_s: 32\n",
      "  edge_dim_v: 1\n",
      "  num_layers_gvp: 21\n",
      "  num_message_layers: 20\n",
      "  num_layers_prediction: 6\n",
      "  dropout: 0.0\n",
      "  use_qkv: True\n",
      "  architecture: interleaved\n",
      "  flexible_loss_scaling: False\n",
      "  k_neighbors: None\n",
      "  k_farthest: None\n",
      "  k_random: None\n",
      "  max_edge_dist: 10.0\n",
      "  num_rbf_3d: 16\n",
      "  num_rbf_seq: 16\n",
      "  no_source_indicator: None\n",
      "  rbf_3d_min: 1.0 (default: 2.0 if None)\n",
      "  rbf_3d_max: 20.0 (default: 350.0 if None)\n",
      "  rbf_3d_spacing: exponential (default: exponential if None)\n",
      "\n",
      "Training parameters:\n",
      "  learning_rate (lr): 0.0001\n",
      "  batch_size (batch): 32\n",
      "  epochs: 500\n",
      "  alpha_min: 1.0\n",
      "  alpha_max: 10.0\n",
      "\n",
      "Time parameters:\n",
      "  t_max: 8.0 (default: 8.0 if None)\n",
      "  t_min: 0.0 (default: 0.0 if None)\n",
      "\n",
      "============================================================\n",
      "DATASET PARAMETER RESOLUTION\n",
      "============================================================\n",
      "split_json: /workspace/datasets/cath-4.2/chain_set_splits.json (source: checkpoint)\n",
      "map_pkl: /workspace/datasets/cath-4.2/chain_set_map_with_b_factors_dssp.pkl (source: checkpoint)\n",
      "use_virtual_node: True (source: checkpoint)\n",
      "max_length: None (source: not specified)\n",
      "use_graph_builder: True (source: default)\n",
      "\n",
      "Graph builder parameter resolution:\n",
      "k_neighbors: None (source: not in checkpoint)\n",
      "k_farthest: None (source: not in checkpoint)\n",
      "k_random: None (source: not in checkpoint)\n",
      "max_edge_dist: 10.0 (source: checkpoint)\n",
      "num_rbf_3d: 16 (source: checkpoint)\n",
      "num_rbf_seq: 16 (source: checkpoint)\n",
      "\n",
      "RBF distance range parameters:\n",
      "rbf_3d_min: 1.0 (source: checkpoint)\n",
      "rbf_3d_max: 20.0 (source: checkpoint)\n",
      "rbf_3d_spacing: exponential (source: checkpoint)\n",
      "\n",
      "Time parameter resolution:\n",
      "t_max: 8.0 (source: checkpoint)\n",
      "t_min: 0.0 (source: default)\n",
      "\n",
      "============================================================\n",
      "MODEL ARCHITECTURE INFERENCE\n",
      "============================================================\n",
      "State dict analysis:\n",
      "  Total keys in state_dict: 818\n",
      "  Embed layer keys found: 0\n",
      "  Interleaved layer keys found: 795\n",
      "  Interleaved layer range: 0 to 61\n",
      "  Example interleaved layer keys: ['gnn.gnn.interleaved_layers.0.gvp.dummy_param', 'gnn.gnn.interleaved_layers.0.gvp.wh.weight', 'gnn.gnn.interleaved_layers.0.gvp.ws.bias']\n",
      "    ... and 792 more interleaved keys\n",
      "  Inferred num_layers: 62\n",
      "\n",
      "Virtual node inference:\n",
      "  Model filename: model_316.pt\n",
      "  Contains 'noVirtual': False\n",
      "  Contains 'Novirtual': False\n",
      "  Inferred use_virtual_node: True\n",
      "  Final use_virtual_node: True (from checkpoint)\n",
      "\n",
      "============================================================\n",
      "FINAL MODEL CREATION PARAMETERS\n",
      "============================================================\n",
      "Architecture detection:\n",
      "  Checkpoint has old 'layers': False\n",
      "  Checkpoint has new 'message_layers': False\n",
      "  Checkpoint has new 'embed': False\n",
      "  Checkpoint has 'interleaved_layers': True\n",
      "  Checkpoint specifies architecture: 'interleaved'\n",
      "Detected interleaved architecture with 62 total layers (indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61])\n",
      "  Warning: Checkpoint args indicate 41 layers (21 embed + 20 message)\n",
      "           but state dict has 62 interleaved layers\n",
      "  Using actual state dict structure for model creation\n",
      "Using interleaved architecture from checkpoint:\n",
      "  num_embed_layers: 21\n",
      "  num_message_layers: 20\n",
      "  architecture: interleaved\n",
      "GVP kwargs (for GNN architecture):\n",
      "  node_dims: (10, 3) (source: default/inferred)\n",
      "  edge_dims: (32, 1) (source: default/inferred)\n",
      "  hidden_dims: (640, 80) (source: default/inferred)\n",
      "  num_layers: 21 (source: default/inferred)\n",
      "  num_embed_layers: 21 (source: default/inferred)\n",
      "  num_message_layers: 20 (source: checkpoint)\n",
      "  architecture: interleaved (source: checkpoint)\n",
      "  use_qkv: True (source: checkpoint)\n",
      "  dropout: 0.0 (source: checkpoint)\n",
      "\n",
      "DFM kwargs (for flow matching):\n",
      "  K: 21 (source: default)\n",
      "  alpha_min: 1.0 (source: checkpoint)\n",
      "  alpha_max: 10.0 (source: checkpoint)\n",
      "  alpha_spacing: 0.01 (source: checkpoint)\n",
      "\n",
      "Additional model parameters:\n",
      "  time_dim: 64 (source: checkpoint)\n",
      "  time_scale: 1.0 (source: checkpoint)\n",
      "  head_hidden: 256 (source: checkpoint)\n",
      "  head_dropout: 0.1 (source: default)\n",
      "  head_depth: 6 (source: checkpoint)\n",
      "  recycle_steps: 1 (source: checkpoint)\n",
      "  time_integration: film (source: checkpoint)\n",
      "  use_time_conditioning: True (source: checkpoint)\n",
      "\n",
      "Creating DFMNodeClassifier with 21 embed layers, 20 message layers, 6 prediction head layers\n",
      "Architecture: interleaved, virtual_node=True (stored in dataset_params)\n",
      "DSSP multitask learning enabled: lambda_dssp_loss=0.5\n",
      "Built interleaved architecture: 21 GVP layers, 20 attention layers\n",
      "\n",
      "============================================================\n",
      "ARCHITECTURE DETECTION AND KEY ADAPTATION\n",
      "============================================================\n",
      "Checkpoint architecture detection:\n",
      "  Has old 'layers' naming: False\n",
      "  Has new 'message_layers': False\n",
      "  Has new 'embed': False\n",
      "  Has 'interleaved_layers': True\n",
      "Model architecture detection:\n",
      "  Has 'message_layers': False\n",
      "  Has 'embed': False\n",
      "  Has 'interleaved_layers': True\n",
      "Key mapping needed: False\n",
      "Mapping strategy: None\n",
      "No key mapping needed - architectures are compatible\n",
      "\n",
      "Final validation after key adaptation:\n",
      "  Missing keys: 0\n",
      "  Unexpected keys: 0\n",
      "\n",
      "Model loaded successfully on cuda\n",
      "\n",
      "============================================================\n",
      "FINAL DATASET PARAMETERS TO BE USED\n",
      "============================================================\n",
      "These parameters will be used for dataset creation:\n",
      "  split_json: /workspace/datasets/cath-4.2/chain_set_splits.json\n",
      "  map_pkl: /workspace/datasets/cath-4.2/chain_set_map_with_b_factors_dssp.pkl\n",
      "  use_virtual_node: True\n",
      "  max_length: None\n",
      "  use_graph_builder: True\n",
      "  k_neighbors: None\n",
      "  k_farthest: None\n",
      "  k_random: None\n",
      "  max_edge_dist: 10.0\n",
      "  num_rbf_3d: 16\n",
      "  num_rbf_seq: 16\n",
      "  no_source_indicator: None\n",
      "  rbf_3d_min: 1.0\n",
      "  rbf_3d_max: 20.0\n",
      "  rbf_3d_spacing: exponential\n",
      "  model_architecture_params: {'hidden_dim': 640, 'hidden_dim_v': 80, 'node_dim_s': 10, 'node_dim_v': 3, 'edge_dim_s': 32, 'edge_dim_v': 1, 'num_layers_gvp': 21, 'num_message_layers': 20, 'num_layers_prediction': 6, 'dropout': 0.0, 'use_qkv': True, 'architecture': 'interleaved', 'flexible_loss_scaling': False}\n",
      "  t_max: 8.0\n",
      "  t_min: 0.0\n",
      "============================================================\n",
      "Model loaded successfully!\n",
      "   Model device: cuda:0\n",
      "   Dataset parameters extracted: 18 parameters\n",
      "split JSON validated: datasets/cath-4.2/chain_set_splits.json\n",
      "map PKL validated: datasets/cath-4.2/chain_set_map_with_b_factors_dssp.pkl\n",
      "\n",
      "Setup complete! Ready to run inference.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD MODEL AND VALIDATE SETUP\n",
    "# =============================================================================\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Created output directory: {output_dir}\")\n",
    "\n",
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Validate model path\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(f\"Model not found at: {model_path}\")\n",
    "print(f\"Model path validated: {model_path}\")\n",
    "\n",
    "# Load model and extract parameters\n",
    "print(\"Loading model...\")\n",
    "try:\n",
    "    model, dataset_params = load_model_distributed(model_path, device, None)\n",
    "    print(\"Model loaded successfully!\")\n",
    "    print(f\"   Model device: {next(model.parameters()).device}\")\n",
    "    if dataset_params:\n",
    "        print(f\"   Dataset parameters extracted: {len(dataset_params)} parameters\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    raise\n",
    "\n",
    "# Validate input files\n",
    "if mode != \"inpainting\" or any(\".\" not in pdb_id for pdb_id in pdb_inputs):\n",
    "    # Only validate dataset files if we might need them\n",
    "    for file_path, name in [(split_json, \"split JSON\"), (map_pkl, \"map PKL\")]:\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Warning: {name} not found at: {file_path}\")\n",
    "            print(\"   (This is okay if using direct PDB files)\")\n",
    "        else:\n",
    "            print(f\"{name} validated: {file_path}\")\n",
    "\n",
    "print(\"\\nSetup complete! Ready to run inference.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5aa63d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def run_sampling_for_protein(pdb_input, mode=\"sampling\", mask_positions=None, \n",
    "                            template_sequence=None, mask_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Run sampling or inpainting for a single protein.\n",
    "    \n",
    "    Args:\n",
    "        pdb_input: PDB ID (e.g., \"1abc\", \"1fcd.C\") or file path\n",
    "        mode: \"sampling\" or \"inpainting\"\n",
    "        mask_positions: List of positions to predict (for inpainting)\n",
    "        template_sequence: Template with 'X' for unknowns (for inpainting)\n",
    "        mask_ratio: Random masking ratio (for inpainting)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with results\n",
    "    \"\"\"\n",
    "    from data.graph_builder import GraphBuilder\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {pdb_input}\")\n",
    "    print(f\"Mode: {mode.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Process input to get protein structure\n",
    "    try:\n",
    "        print(\"Processing input structure...\")\n",
    "        entry, temp_files = process_input_specification(pdb_input, verbose=verbose)\n",
    "        print(f\"Structure loaded: {entry['name']}\")\n",
    "        print(f\"   Length: {len(entry['seq'])} residues\")\n",
    "        print(f\"   Source: {entry['source']}\")\n",
    "        \n",
    "        # Build graph using parameters from checkpoint (following training/sample_utils.py pattern)\n",
    "        print(\"Building molecular graph...\")\n",
    "        \n",
    "        # Prepare graph builder parameters exactly like in training/sample_utils.py\n",
    "        graph_builder_kwargs = {\n",
    "            'k': dataset_params.get('k_neighbors'),\n",
    "            'k_farthest': dataset_params.get('k_farthest'),\n",
    "            'k_random': dataset_params.get('k_random'),\n",
    "            'max_edge_dist': dataset_params.get('max_edge_dist'),\n",
    "            'num_rbf_3d': dataset_params.get('num_rbf_3d'),\n",
    "            'num_rbf_seq': dataset_params.get('num_rbf_seq'),\n",
    "            'use_virtual_node': dataset_params.get('use_virtual_node', True),\n",
    "            'no_source_indicator': dataset_params.get('no_source_indicator', False),\n",
    "            # RBF distance range parameters\n",
    "            'rbf_3d_min': dataset_params.get('rbf_3d_min'),\n",
    "            'rbf_3d_max': dataset_params.get('rbf_3d_max'),\n",
    "            'rbf_3d_spacing': dataset_params.get('rbf_3d_spacing'),\n",
    "            'verbose': False  # Reduce clutter\n",
    "        }\n",
    "        \n",
    "        # Validate RBF parameters (like in training code)\n",
    "        if (graph_builder_kwargs['rbf_3d_min'] is None or \n",
    "            graph_builder_kwargs['rbf_3d_max'] is None or \n",
    "            graph_builder_kwargs['rbf_3d_spacing'] is None):\n",
    "            raise RuntimeError(\n",
    "                f\"RBF parameters missing from checkpoint. \"\n",
    "                f\"Got: rbf_3d_min={graph_builder_kwargs['rbf_3d_min']}, \"\n",
    "                f\"rbf_3d_max={graph_builder_kwargs['rbf_3d_max']}, \"\n",
    "                f\"rbf_3d_spacing={graph_builder_kwargs['rbf_3d_spacing']}\"\n",
    "            )\n",
    "        \n",
    "        # Remove None values to use GraphBuilder defaults (like in training code)\n",
    "        graph_builder_kwargs = {k: v for k, v in graph_builder_kwargs.items() if v is not None}\n",
    "        \n",
    "        builder = GraphBuilder(**graph_builder_kwargs)\n",
    "        graph_data = builder.build_from_dict(entry, time_param=0.0)\n",
    "        graph_data = graph_data.to(device)\n",
    "        print(f\"Graph built: {graph_data.num_nodes} nodes, {graph_data.num_edges} edges\")\n",
    "        \n",
    "        # Run sampling or inpainting\n",
    "        if mode == \"sampling\":\n",
    "            print(\"Running sequence sampling...\")\n",
    "            \n",
    "            # Handle ensemble sampling if requested\n",
    "            if ensemble_size > 1:\n",
    "                from training.sample_utils import create_structural_ensemble, sample_with_ensemble_consensus\n",
    "                \n",
    "                print(f\"   Creating ensemble of {ensemble_size} structural variants...\")\n",
    "                batched_ensemble = create_structural_ensemble(\n",
    "                    entry, ensemble_size=ensemble_size,\n",
    "                    structure_noise_mag_std=structure_noise_mag_std,\n",
    "                    uncertainty_struct_noise_scaling=uncertainty_struct_noise_scaling,\n",
    "                    device=device, args=None, dataset_params=dataset_params\n",
    "                )\n",
    "                \n",
    "                print(\"   Running ensemble sampling with consensus...\")\n",
    "                predicted_sequence = sample_with_ensemble_consensus(\n",
    "                    model, batched_ensemble, T=max_time, t_min=min_time, steps=steps,\n",
    "                    K=21, consensus_strength=ensemble_consensus_strength, device=device,\n",
    "                    use_virtual_node=dataset_params.get('use_virtual_node', True), args=None\n",
    "                )\n",
    "                \n",
    "                # Convert indices to sequence (IDX_TO_AA is a list, not dict)\n",
    "                predicted_seq_str = ''.join([THREE_TO_ONE.get(IDX_TO_AA[idx] if 0 <= idx < len(IDX_TO_AA) else 'UNK', 'X') for idx in predicted_sequence])\n",
    "                final_probabilities = None  # Ensemble doesn't return probabilities\n",
    "                eval_metrics = {}\n",
    "                \n",
    "            else:\n",
    "                # Single structure sampling\n",
    "                final_probabilities, predicted_sequence_indices, eval_metrics = sample_chain(\n",
    "                    model, graph_data, dataset=None, structure_idx=None,\n",
    "                    T=max_time, t_min=min_time, steps=steps, K=21, verbose=False, args=None  # Reduced verbosity\n",
    "                )\n",
    "                \n",
    "                # Convert indices to amino acid sequence (IDX_TO_AA is a list, not dict)\n",
    "                predicted_seq_str = ''.join([THREE_TO_ONE.get(IDX_TO_AA[idx] if 0 <= idx < len(IDX_TO_AA) else 'UNK', 'X') for idx in predicted_sequence_indices])\n",
    "                \n",
    "        elif mode == \"inpainting\":\n",
    "            print(\"Running sequence inpainting...\")\n",
    "            \n",
    "            final_probabilities, predicted_sequence_indices, inpainting_mask, alignment_info, eval_metrics = sample_chain_inpainting(\n",
    "                model, graph_data, T=max_time, t_min=min_time, steps=steps, K=21,\n",
    "                full_sequence=entry['seq'], structure_sequence=entry['seq'],\n",
    "                mask_positions=mask_positions, known_sequence=template_sequence,\n",
    "                mask_ratio=mask_ratio, verbose=False, args=None  # Reduced verbosity\n",
    "            )\n",
    "            \n",
    "            # Convert indices to amino acid sequence (IDX_TO_AA is a list, not dict)\n",
    "            predicted_seq_str = ''.join([THREE_TO_ONE.get(IDX_TO_AA[idx] if 0 <= idx < len(IDX_TO_AA) else 'UNK', 'X') for idx in predicted_sequence_indices])\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown mode: {mode}\")\n",
    "        \n",
    "        # Prepare results\n",
    "        result = {\n",
    "            'protein_id': entry['name'],\n",
    "            'original_sequence': entry['seq'],\n",
    "            'predicted_sequence': predicted_seq_str,  # Now properly converted to amino acid letters\n",
    "            'sequence_length': len(predicted_seq_str),\n",
    "            'mode': mode,\n",
    "            'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        \n",
    "        # Add evaluation metrics if available\n",
    "        if eval_metrics:\n",
    "            result.update(eval_metrics)\n",
    "            \n",
    "        # Add inpainting-specific info\n",
    "        if mode == \"inpainting\":\n",
    "            if mask_positions:\n",
    "                result['mask_positions'] = mask_positions\n",
    "            if template_sequence:\n",
    "                result['template_sequence'] = template_sequence\n",
    "            result['mask_ratio'] = mask_ratio\n",
    "        \n",
    "        # Clean up temp files\n",
    "        for temp_file in temp_files:\n",
    "            try:\n",
    "                if os.path.exists(temp_file):\n",
    "                    os.remove(temp_file)\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdb_input}: {e}\")\n",
    "        return None\n",
    "\n",
    "def display_results(results):\n",
    "    \"\"\"Display results in a nice format.\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"RESULTS SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        if result is None:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nProtein {i}: {result['protein_id']}\")\n",
    "        print(f\"   Mode: {result['mode'].upper()}\")\n",
    "        print(f\"   Length: {result['sequence_length']} residues\")\n",
    "        \n",
    "        if result['mode'] == 'inpainting' and 'mask_positions' in result:\n",
    "            print(f\"   Masked positions: {result['mask_positions']}\")\n",
    "            \n",
    "        print(f\"   Original : {result['original_sequence']}\")\n",
    "        print(f\"   Predicted: {result['predicted_sequence']}\")\n",
    "        \n",
    "        if 'accuracy' in result:\n",
    "            print(f\"   Accuracy: {result['accuracy']:.3f}\")\n",
    "        if 'confidence' in result:\n",
    "            print(f\"   Confidence: {result['confidence']:.3f}\")\n",
    "            \n",
    "    print(f\"\\n{'='*80}\")\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b973a770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting protein sequence inference...\n",
      "   Processing 2 protein(s)\n",
      "   Mode: SAMPLING\n",
      "\n",
      "Processing protein 1/2: 1fcd.C\n",
      "\n",
      "============================================================\n",
      "Processing: 1fcd.C\n",
      "Mode: SAMPLING\n",
      "============================================================\n",
      "Processing input structure...\n",
      "Detected PDB ID with chain: 1fcd.C\n",
      "Creating temporary directory: /tmp/tmpgojrs70w\n",
      "Downloading 1fcd from RCSB PDB...\n",
      "Downloaded to: /tmp/tmpgojrs70w/1fcd.pdb\n",
      "Extracting chain C...\n",
      "Extracted chain C to: /tmp/tmpgojrs70w/1fcd_chainC.pdb\n",
      "Converting to internal dictionary format...\n",
      "Successfully processed pdb_id_with_chain: 1FCD.C\n",
      "Structure loaded: 1FCD.C\n",
      "   Length: 174 residues\n",
      "   Source: pdb\n",
      "Building molecular graph...\n",
      "Auto-detected project root: /home/t-alptartici/inverse-folding\n",
      "Using cache directory: /home/t-alptartici/inverse-folding/rbf_cache\n",
      "RBF Manager initialized with 3D range: 1.0-20.0Ã…\n",
      "RBF 3D filename: rbf_3d_features16_centers1.0to20.0_table40.0_res0.001_exponential.npy\n",
      "RBF cache directory: /home/t-alptartici/inverse-folding/rbf_cache\n",
      "Found existing sequence RBF table: /home/t-alptartici/inverse-folding/rbf_cache/rbf_seq_features16_min2_max1000_seqrange-2000to2000.npy\n",
      "Loading sequence RBF table from disk into cpu memory...\n",
      "Cached sequence RBF table: (4001, 16), 0.2 MB on cpu\n",
      "Found existing 3D RBF table: /home/t-alptartici/inverse-folding/rbf_cache/rbf_3d_features16_centers1.0to20.0_table40.0_res0.001_exponential.npy\n",
      "Loading 3D RBF table from disk into cpu memory...\n",
      "Cached 3D RBF table: (40001, 16), 2.4 MB on cpu\n",
      "Graph built: 175 nodes, 3367 edges\n",
      "Running sequence sampling...\n",
      "   Creating ensemble of 3 structural variants...\n",
      "Creating ensemble of 3 replicas\n",
      "  Noise std: 0.1 Ã…\n",
      "Auto-detected project root: /home/t-alptartici/inverse-folding\n",
      "Using cache directory: /home/t-alptartici/inverse-folding/rbf_cache\n",
      "RBF Manager initialized with 3D range: 1.0-20.0Ã…\n",
      "RBF 3D filename: rbf_3d_features16_centers1.0to20.0_table40.0_res0.001_exponential.npy\n",
      "RBF cache directory: /home/t-alptartici/inverse-folding/rbf_cache\n",
      "Auto-detected project root: /home/t-alptartici/inverse-folding\n",
      "Using cache directory: /home/t-alptartici/inverse-folding/rbf_cache\n",
      "RBF Manager initialized with 3D range: 1.0-20.0Ã…\n",
      "RBF 3D filename: rbf_3d_features16_centers1.0to20.0_table40.0_res0.001_exponential.npy\n",
      "RBF cache directory: /home/t-alptartici/inverse-folding/rbf_cache\n",
      "Auto-detected project root: /home/t-alptartici/inverse-folding\n",
      "Using cache directory: /home/t-alptartici/inverse-folding/rbf_cache\n",
      "RBF Manager initialized with 3D range: 1.0-20.0Ã…\n",
      "RBF 3D filename: rbf_3d_features16_centers1.0to20.0_table40.0_res0.001_exponential.npy\n",
      "RBF cache directory: /home/t-alptartici/inverse-folding/rbf_cache\n",
      "Auto-detected project root: /home/t-alptartici/inverse-folding\n",
      "Using cache directory: /home/t-alptartici/inverse-folding/rbf_cache\n",
      "RBF Manager initialized with 3D range: 1.0-20.0Ã…\n",
      "RBF 3D filename: rbf_3d_features16_centers1.0to20.0_table40.0_res0.001_exponential.npy\n",
      "RBF cache directory: /home/t-alptartici/inverse-folding/rbf_cache\n",
      "   Running ensemble sampling with consensus...\n",
      "\n",
      "Sampling with 3-member ensemble, consensus_strength=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ensemble sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:06<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 1fcd.C\n",
      "\n",
      "Processing protein 2/2: 1h2s.B\n",
      "\n",
      "============================================================\n",
      "Processing: 1h2s.B\n",
      "Mode: SAMPLING\n",
      "============================================================\n",
      "Processing input structure...\n",
      "Detected PDB ID with chain: 1h2s.B\n",
      "Creating temporary directory: /tmp/tmp3g_hwwwe\n",
      "Downloading 1h2s from RCSB PDB...\n",
      "Downloaded to: /tmp/tmp3g_hwwwe/1h2s.pdb\n",
      "Extracting chain B...\n",
      "Extracted chain B to: /tmp/tmp3g_hwwwe/1h2s_chainB.pdb\n",
      "Converting to internal dictionary format...\n",
      "Successfully processed pdb_id_with_chain: 1H2S.B\n",
      "Structure loaded: 1H2S.B\n",
      "   Length: 60 residues\n",
      "   Source: pdb\n",
      "Building molecular graph...\n",
      "Auto-detected project root: /home/t-alptartici/inverse-folding\n",
      "Using cache directory: /home/t-alptartici/inverse-folding/rbf_cache\n",
      "RBF Manager initialized with 3D range: 1.0-20.0Ã…\n",
      "RBF 3D filename: rbf_3d_features16_centers1.0to20.0_table40.0_res0.001_exponential.npy\n",
      "RBF cache directory: /home/t-alptartici/inverse-folding/rbf_cache\n",
      "Graph built: 61 nodes, 929 edges\n",
      "Running sequence sampling...\n",
      "   Creating ensemble of 3 structural variants...\n",
      "Creating ensemble of 3 replicas\n",
      "  Noise std: 0.1 Ã…\n",
      "Auto-detected project root: /home/t-alptartici/inverse-folding\n",
      "Using cache directory: /home/t-alptartici/inverse-folding/rbf_cache\n",
      "RBF Manager initialized with 3D range: 1.0-20.0Ã…\n",
      "RBF 3D filename: rbf_3d_features16_centers1.0to20.0_table40.0_res0.001_exponential.npy\n",
      "RBF cache directory: /home/t-alptartici/inverse-folding/rbf_cache\n",
      "Auto-detected project root: /home/t-alptartici/inverse-folding\n",
      "Using cache directory: /home/t-alptartici/inverse-folding/rbf_cache\n",
      "RBF Manager initialized with 3D range: 1.0-20.0Ã…\n",
      "RBF 3D filename: rbf_3d_features16_centers1.0to20.0_table40.0_res0.001_exponential.npy\n",
      "RBF cache directory: /home/t-alptartici/inverse-folding/rbf_cache\n",
      "Auto-detected project root: /home/t-alptartici/inverse-folding\n",
      "Using cache directory: /home/t-alptartici/inverse-folding/rbf_cache\n",
      "RBF Manager initialized with 3D range: 1.0-20.0Ã…\n",
      "RBF 3D filename: rbf_3d_features16_centers1.0to20.0_table40.0_res0.001_exponential.npy\n",
      "RBF cache directory: /home/t-alptartici/inverse-folding/rbf_cache\n",
      "Auto-detected project root: /home/t-alptartici/inverse-folding\n",
      "Using cache directory: /home/t-alptartici/inverse-folding/rbf_cache\n",
      "RBF Manager initialized with 3D range: 1.0-20.0Ã…\n",
      "RBF 3D filename: rbf_3d_features16_centers1.0to20.0_table40.0_res0.001_exponential.npy\n",
      "RBF cache directory: /home/t-alptartici/inverse-folding/rbf_cache\n",
      "   Running ensemble sampling with consensus...\n",
      "\n",
      "Sampling with 3-member ensemble, consensus_strength=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ensemble sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:05<00:00,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 1h2s.B\n",
      "\n",
      "================================================================================\n",
      "RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Protein 1: 1FCD.C\n",
      "   Mode: SAMPLING\n",
      "   Length: 174 residues\n",
      "   Original : EPTAEMLTNNCAGCHGTHGNSVGPASPSIAQMDPMVFVEVMEGFKSGEIASTIMGRIAKGYSTADFEKMAGYFKQQTYQPAKQSFDTALADTGAKLHDKYCEKCHVEGGKPLADEEDYHILAGQWTPYLQYAMSDFREERRPMEKKMASKLRELLKAEGDAGLDALFAFYASQQ\n",
      "   Predicted: MKSAEELAAGCAKCHGKGGVAPPPSAPPLGNVDPELLLKLLSAAKSGTIPSPSLPEIASKYNEEEAEELAAYLSKLTPVPVKVEYDPELAEKGKELFKKYCAKCHKDGGRPVPDGEGYIPLGGANPARLKKLFELIRNGKIPVDEECREKLDEKLLEKGEAGLEAILAYLASLR\n",
      "\n",
      "Protein 2: 1H2S.B\n",
      "   Mode: SAMPLING\n",
      "   Length: 60 residues\n",
      "   Original : GAVFIFVGALTVLFGAIAYGEVTAAAATGDAAAVQEAAVSAILGLIILLGINLGLVAATL\n",
      "   Predicted: MLLLLLFLLLFLLALLVGAAVVVAALATGDVAAVLVALLLAVLGLLLLLGVAAALERLLG\n",
      "\n",
      "================================================================================\n",
      "Results saved to: ./inference_output/sampling_results_20250905_212008.csv\n",
      "\n",
      "Results DataFrame:\n",
      "\n",
      "Inference complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# RUN INFERENCE - MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Starting protein sequence inference...\")\n",
    "print(f\"   Processing {len(pdb_inputs)} protein(s)\")\n",
    "print(f\"   Mode: {mode.upper()}\")\n",
    "\n",
    "# Store all results\n",
    "all_results = []\n",
    "\n",
    "# Process each protein\n",
    "for i, pdb_input in enumerate(pdb_inputs, 1):\n",
    "    print(f\"\\nProcessing protein {i}/{len(pdb_inputs)}: {pdb_input}\")\n",
    "    \n",
    "    result = run_sampling_for_protein(\n",
    "        pdb_input=pdb_input,\n",
    "        mode=mode,\n",
    "        mask_positions=mask_positions if mode == \"inpainting\" else None,\n",
    "        template_sequence=template_sequence if mode == \"inpainting\" else None,\n",
    "        mask_ratio=mask_ratio if mode == \"inpainting\" else None\n",
    "    )\n",
    "    \n",
    "    if result:\n",
    "        all_results.append(result)\n",
    "        print(f\"Successfully processed {pdb_input}\")\n",
    "    else:\n",
    "        print(f\"Failed to process {pdb_input}\")\n",
    "\n",
    "# Display and save results\n",
    "if all_results:\n",
    "    display_results(all_results)\n",
    "    \n",
    "    # Save to CSV\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    csv_filename = f\"{output_dir}/sampling_results_{timestamp}.csv\"\n",
    "    \n",
    "    df = pd.DataFrame(all_results)\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    print(f\"Results saved to: {csv_filename}\")\n",
    "    \n",
    "    # Display DataFrame\n",
    "    cols_to_display = ['protein_id', 'original_sequence', 'predicted_sequence', 'confidence', 'accuracy', 'top3_accuracy']\n",
    "    print(f\"\\nResults DataFrame:\")\n",
    "    try:\n",
    "        display(df[cols_to_display])\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "else:\n",
    "    print(\"No successful results to save.\")\n",
    "\n",
    "print(\"\\nInference complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f90f89",
   "metadata": {},
   "source": [
    "## Usage Examples & Tips\n",
    "\n",
    "### Example 1: Basic Sampling\n",
    "```python\n",
    "# Simple sequence generation\n",
    "mode = \"sampling\"\n",
    "pdb_inputs = [\"1abc\", \"1fcd.C\"]\n",
    "flow_temp = 0.2\n",
    "steps = 20\n",
    "```\n",
    "\n",
    "### Example 2: Inpainting with Specific Positions\n",
    "```python\n",
    "# Predict specific positions\n",
    "mode = \"inpainting\"\n",
    "pdb_inputs = [\"1fcd.C\"]\n",
    "mask_positions = [10, 15, 20, 25]  # Predict these positions\n",
    "template_sequence = None\n",
    "```\n",
    "\n",
    "### Example 3: Template-Based Inpainting\n",
    "```python\n",
    "# Use template with X for unknowns\n",
    "mode = \"inpainting\"\n",
    "pdb_inputs = [\"1fcd.C\"]\n",
    "mask_positions = None\n",
    "template_sequence = \"ACDEFXHIKLXNPQXSTVWY\"  # X marks positions to predict\n",
    "```\n",
    "\n",
    "### Example 4: High-Quality Ensemble Sampling\n",
    "```python\n",
    "# Generate multiple structural variants for consensus\n",
    "mode = \"sampling\"\n",
    "pdb_inputs = [\"1fcd.C\"]\n",
    "ensemble_size = 5\n",
    "ensemble_consensus_strength = 1.0\n",
    "structure_noise_mag_std = 0.2\n",
    "```\n",
    "\n",
    "### Parameter Guidelines:\n",
    "- **flow_temp**: 0.1 (conservative) â†’ 1.0 (diverse)\n",
    "- **steps**: 10 (fast) â†’ 50 (high quality)\n",
    "- **ensemble_size**: 1 (single) â†’ 10 (consensus)\n",
    "- **structure_noise_mag_std**: 0.0 (no noise) â†’ 2.0 Ã… (high diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0e9d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Quick test cell ready (uncomment to run)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# QUICK TEST (OPTIONAL) - Test with a small protein\n",
    "# =============================================================================\n",
    "\n",
    "# Uncomment and run this cell to test with a small protein first\n",
    "\"\"\"\n",
    "print(\"Running quick test with a small protein...\")\n",
    "\n",
    "# Test parameters (smaller/faster)\n",
    "test_pdb = \"1fcd.C\"  # Small test protein\n",
    "test_mode = \"sampling\"\n",
    "test_flow_temp = 0.2\n",
    "test_steps = 10\n",
    "\n",
    "# Run test\n",
    "test_result = run_sampling_for_protein(\n",
    "    pdb_input=test_pdb,\n",
    "    mode=test_mode,\n",
    "    mask_positions=None,\n",
    "    template_sequence=None,\n",
    "    mask_ratio=0.15\n",
    ")\n",
    "\n",
    "if test_result:\n",
    "    print(\"Quick test successful!\")\n",
    "    print(f\"   Test protein: {test_result['protein_id']}\")\n",
    "    print(f\"   Predicted sequence: {test_result['predicted_sequence']}\")\n",
    "else:\n",
    "    print(\"Quick test failed!\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"Quick test cell ready (uncomment to run)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352b18de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf245bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RESULTS ANALYSIS (OPTIONAL)\n",
    "# =============================================================================\n",
    "\n",
    "# This cell provides additional analysis of the results\n",
    "if 'all_results' in locals() and all_results:\n",
    "    print(\"Additional Results Analysis\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for result in all_results:\n",
    "        if result is None:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n{result['protein_id']}:\")\n",
    "        \n",
    "        # Sequence comparison\n",
    "        orig_seq = result['original_sequence']\n",
    "        pred_seq = result['predicted_sequence']\n",
    "        \n",
    "        # Calculate sequence identity if we have original\n",
    "        if orig_seq and pred_seq and len(orig_seq) == len(pred_seq):\n",
    "            matches = sum(1 for a, b in zip(orig_seq, pred_seq) if a == b)\n",
    "            identity = matches / len(orig_seq)\n",
    "            print(f\"   Sequence Identity: {identity:.3f} ({matches}/{len(orig_seq)} matches)\")\n",
    "            \n",
    "            # Show differences\n",
    "            differences = []\n",
    "            for i, (orig, pred) in enumerate(zip(orig_seq, pred_seq)):\n",
    "                if orig != pred:\n",
    "                    differences.append(f\"{i+1}{orig}â†’{pred}\")\n",
    "            \n",
    "            if differences and len(differences) <= 10:\n",
    "                print(f\"   Differences: {', '.join(differences)}\")\n",
    "            elif differences:\n",
    "                print(f\"   Differences: {len(differences)} changes (showing first 10): {', '.join(differences[:10])}...\")\n",
    "        \n",
    "        # Amino acid composition analysis\n",
    "        if pred_seq:\n",
    "            aa_counts = {}\n",
    "            for aa in pred_seq:\n",
    "                aa_counts[aa] = aa_counts.get(aa, 0) + 1\n",
    "            \n",
    "            # Show most common amino acids\n",
    "            sorted_aa = sorted(aa_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "            composition_str = \", \".join([f\"{aa}({count})\" for aa, count in sorted_aa])\n",
    "            print(f\"   Top amino acids: {composition_str}\")\n",
    "            \n",
    "        # Additional metrics if available\n",
    "        if 'confidence' in result:\n",
    "            print(f\"   Average confidence: {result['confidence']:.3f}\")\n",
    "        if 'entropy' in result:\n",
    "            print(f\"   Average entropy: {result['entropy']:.3f}\")\n",
    "            \n",
    "    print(f\"\\nAll results saved to CSV files in: {output_dir}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No results to analyze. Run the inference first!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inv_fold_dir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
